{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [[0], [0], [0], [1]]\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기화\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        print('초기화')\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #print('입력')\n",
    "        output = self.linear(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "model = LogisticRegression(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0/100000] Loss:0.7336\n",
      "[Epoch:1000/100000] Loss:0.6496\n",
      "[Epoch:2000/100000] Loss:0.6002\n",
      "[Epoch:3000/100000] Loss:0.5674\n",
      "[Epoch:4000/100000] Loss:0.5428\n",
      "[Epoch:5000/100000] Loss:0.5227\n",
      "[Epoch:6000/100000] Loss:0.5051\n",
      "[Epoch:7000/100000] Loss:0.4892\n",
      "[Epoch:8000/100000] Loss:0.4746\n",
      "[Epoch:9000/100000] Loss:0.4611\n",
      "[Epoch:10000/100000] Loss:0.4484\n",
      "[Epoch:11000/100000] Loss:0.4365\n",
      "[Epoch:12000/100000] Loss:0.4252\n",
      "[Epoch:13000/100000] Loss:0.4147\n",
      "[Epoch:14000/100000] Loss:0.4046\n",
      "[Epoch:15000/100000] Loss:0.3952\n",
      "[Epoch:16000/100000] Loss:0.3862\n",
      "[Epoch:17000/100000] Loss:0.3776\n",
      "[Epoch:18000/100000] Loss:0.3695\n",
      "[Epoch:19000/100000] Loss:0.3618\n",
      "[Epoch:20000/100000] Loss:0.3544\n",
      "[Epoch:21000/100000] Loss:0.3474\n",
      "[Epoch:22000/100000] Loss:0.3406\n",
      "[Epoch:23000/100000] Loss:0.3342\n",
      "[Epoch:24000/100000] Loss:0.328\n",
      "[Epoch:25000/100000] Loss:0.3221\n",
      "[Epoch:26000/100000] Loss:0.3164\n",
      "[Epoch:27000/100000] Loss:0.3109\n",
      "[Epoch:28000/100000] Loss:0.3057\n",
      "[Epoch:29000/100000] Loss:0.3006\n",
      "[Epoch:30000/100000] Loss:0.2957\n",
      "[Epoch:31000/100000] Loss:0.291\n",
      "[Epoch:32000/100000] Loss:0.2865\n",
      "[Epoch:33000/100000] Loss:0.2821\n",
      "[Epoch:34000/100000] Loss:0.2779\n",
      "[Epoch:35000/100000] Loss:0.2738\n",
      "[Epoch:36000/100000] Loss:0.2698\n",
      "[Epoch:37000/100000] Loss:0.2659\n",
      "[Epoch:38000/100000] Loss:0.2622\n",
      "[Epoch:39000/100000] Loss:0.2586\n",
      "[Epoch:40000/100000] Loss:0.2551\n",
      "[Epoch:41000/100000] Loss:0.2517\n",
      "[Epoch:42000/100000] Loss:0.2483\n",
      "[Epoch:43000/100000] Loss:0.2451\n",
      "[Epoch:44000/100000] Loss:0.242\n",
      "[Epoch:45000/100000] Loss:0.2389\n",
      "[Epoch:46000/100000] Loss:0.236\n",
      "[Epoch:47000/100000] Loss:0.2331\n",
      "[Epoch:48000/100000] Loss:0.2303\n",
      "[Epoch:49000/100000] Loss:0.2275\n",
      "[Epoch:50000/100000] Loss:0.2248\n",
      "[Epoch:51000/100000] Loss:0.2222\n",
      "[Epoch:52000/100000] Loss:0.2197\n",
      "[Epoch:53000/100000] Loss:0.2172\n",
      "[Epoch:54000/100000] Loss:0.2147\n",
      "[Epoch:55000/100000] Loss:0.2124\n",
      "[Epoch:56000/100000] Loss:0.2101\n",
      "[Epoch:57000/100000] Loss:0.2078\n",
      "[Epoch:58000/100000] Loss:0.2056\n",
      "[Epoch:59000/100000] Loss:0.2034\n",
      "[Epoch:60000/100000] Loss:0.2013\n",
      "[Epoch:61000/100000] Loss:0.1992\n",
      "[Epoch:62000/100000] Loss:0.1971\n",
      "[Epoch:63000/100000] Loss:0.1952\n",
      "[Epoch:64000/100000] Loss:0.1932\n",
      "[Epoch:65000/100000] Loss:0.1913\n",
      "[Epoch:66000/100000] Loss:0.1894\n",
      "[Epoch:67000/100000] Loss:0.1876\n",
      "[Epoch:68000/100000] Loss:0.1858\n",
      "[Epoch:69000/100000] Loss:0.184\n",
      "[Epoch:70000/100000] Loss:0.1823\n",
      "[Epoch:71000/100000] Loss:0.1806\n",
      "[Epoch:72000/100000] Loss:0.1789\n",
      "[Epoch:73000/100000] Loss:0.1773\n",
      "[Epoch:74000/100000] Loss:0.1757\n",
      "[Epoch:75000/100000] Loss:0.1741\n",
      "[Epoch:76000/100000] Loss:0.1725\n",
      "[Epoch:77000/100000] Loss:0.171\n",
      "[Epoch:78000/100000] Loss:0.1695\n",
      "[Epoch:79000/100000] Loss:0.168\n",
      "[Epoch:80000/100000] Loss:0.1666\n",
      "[Epoch:81000/100000] Loss:0.1652\n",
      "[Epoch:82000/100000] Loss:0.1638\n",
      "[Epoch:83000/100000] Loss:0.1624\n",
      "[Epoch:84000/100000] Loss:0.161\n",
      "[Epoch:85000/100000] Loss:0.1597\n",
      "[Epoch:86000/100000] Loss:0.1584\n",
      "[Epoch:87000/100000] Loss:0.1571\n",
      "[Epoch:88000/100000] Loss:0.1558\n",
      "[Epoch:89000/100000] Loss:0.1546\n",
      "[Epoch:90000/100000] Loss:0.1534\n",
      "[Epoch:91000/100000] Loss:0.1522\n",
      "[Epoch:92000/100000] Loss:0.151\n",
      "[Epoch:93000/100000] Loss:0.1498\n",
      "[Epoch:94000/100000] Loss:0.1487\n",
      "[Epoch:95000/100000] Loss:0.1475\n",
      "[Epoch:96000/100000] Loss:0.1464\n",
      "[Epoch:97000/100000] Loss:0.1453\n",
      "[Epoch:98000/100000] Loss:0.1442\n",
      "[Epoch:99000/100000] Loss:0.1431\n"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "#metric: 분류: 정확도, 회귀: mse, mae(정답 - 예측값)\n",
    "total = 0\n",
    "correct = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    prediction = model(x)\n",
    "    \n",
    "    loss = criterion(prediction, y)# 정답과 예측된 값 오차 계산\n",
    "    \n",
    "    optimizer.zero_grad() #gradient를 계산하면 gradient를 저장을 하는데 저장된 값을 비워야함\n",
    "    loss.backward() # 역전파로 gradient 계산\n",
    "    optimizer.step() #gradient descent 계산이 된다.\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'[Epoch:{epoch}/{epochs}] Loss:{round(loss.item(), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [[0], [1], [1], [1]]\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = self.linear(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "model = LogisticRegression(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0/100000] Loss: 0.604\n",
      "[Epoch:10000/100000] Loss: 0.3454\n",
      "[Epoch:20000/100000] Loss: 0.2689\n",
      "[Epoch:30000/100000] Loss: 0.2186\n",
      "[Epoch:40000/100000] Loss: 0.1834\n",
      "[Epoch:50000/100000] Loss: 0.1574\n",
      "[Epoch:60000/100000] Loss: 0.1376\n",
      "[Epoch:70000/100000] Loss: 0.1219\n",
      "[Epoch:80000/100000] Loss: 0.1093\n",
      "[Epoch:90000/100000] Loss: 0.0989\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100000\n",
    "total = 0\n",
    "correct = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    prediction = model(x)\n",
    "    loss = criterion(prediction, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10000 == 0:\n",
    "        print(f'[Epoch:{epoch}/{epochs}] Loss: {round(loss.item(), 4)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [[0], [1], [1], [0]]\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(input_size, input_size)\n",
    "        self.linear3 = nn.Linear(10, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.linear3(X)\n",
    "        output = self.sigmoid(X)\n",
    "        return output\n",
    "\n",
    "model = LogisticRegression(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0/100000] Loss: 0.694\n",
      "[Epoch:1000/100000] Loss: 0.6923\n",
      "[Epoch:2000/100000] Loss: 0.6906\n",
      "[Epoch:3000/100000] Loss: 0.6888\n",
      "[Epoch:4000/100000] Loss: 0.6868\n",
      "[Epoch:5000/100000] Loss: 0.6844\n",
      "[Epoch:6000/100000] Loss: 0.6815\n",
      "[Epoch:7000/100000] Loss: 0.6778\n",
      "[Epoch:8000/100000] Loss: 0.6733\n",
      "[Epoch:9000/100000] Loss: 0.6677\n",
      "[Epoch:10000/100000] Loss: 0.6609\n",
      "[Epoch:11000/100000] Loss: 0.6548\n",
      "[Epoch:12000/100000] Loss: 0.6477\n",
      "[Epoch:13000/100000] Loss: 0.6397\n",
      "[Epoch:14000/100000] Loss: 0.6306\n",
      "[Epoch:15000/100000] Loss: 0.6203\n",
      "[Epoch:16000/100000] Loss: 0.6105\n",
      "[Epoch:17000/100000] Loss: 0.5998\n",
      "[Epoch:18000/100000] Loss: 0.5884\n",
      "[Epoch:19000/100000] Loss: 0.5762\n",
      "[Epoch:20000/100000] Loss: 0.5632\n",
      "[Epoch:21000/100000] Loss: 0.5503\n",
      "[Epoch:22000/100000] Loss: 0.5378\n",
      "[Epoch:23000/100000] Loss: 0.5252\n",
      "[Epoch:24000/100000] Loss: 0.5124\n",
      "[Epoch:25000/100000] Loss: 0.4996\n",
      "[Epoch:26000/100000] Loss: 0.4869\n",
      "[Epoch:27000/100000] Loss: 0.4743\n",
      "[Epoch:28000/100000] Loss: 0.4617\n",
      "[Epoch:29000/100000] Loss: 0.4492\n",
      "[Epoch:30000/100000] Loss: 0.4368\n",
      "[Epoch:31000/100000] Loss: 0.4244\n",
      "[Epoch:32000/100000] Loss: 0.4121\n",
      "[Epoch:33000/100000] Loss: 0.3995\n",
      "[Epoch:34000/100000] Loss: 0.3867\n",
      "[Epoch:35000/100000] Loss: 0.3733\n",
      "[Epoch:36000/100000] Loss: 0.3599\n",
      "[Epoch:37000/100000] Loss: 0.3467\n",
      "[Epoch:38000/100000] Loss: 0.3332\n",
      "[Epoch:39000/100000] Loss: 0.3194\n",
      "[Epoch:40000/100000] Loss: 0.3053\n",
      "[Epoch:41000/100000] Loss: 0.2911\n",
      "[Epoch:42000/100000] Loss: 0.2769\n",
      "[Epoch:43000/100000] Loss: 0.2626\n",
      "[Epoch:44000/100000] Loss: 0.2485\n",
      "[Epoch:45000/100000] Loss: 0.2347\n",
      "[Epoch:46000/100000] Loss: 0.2213\n",
      "[Epoch:47000/100000] Loss: 0.2084\n",
      "[Epoch:48000/100000] Loss: 0.1961\n",
      "[Epoch:49000/100000] Loss: 0.1845\n",
      "[Epoch:50000/100000] Loss: 0.1734\n",
      "[Epoch:51000/100000] Loss: 0.1631\n",
      "[Epoch:52000/100000] Loss: 0.1535\n",
      "[Epoch:53000/100000] Loss: 0.1445\n",
      "[Epoch:54000/100000] Loss: 0.1361\n",
      "[Epoch:55000/100000] Loss: 0.1284\n",
      "[Epoch:56000/100000] Loss: 0.1212\n",
      "[Epoch:57000/100000] Loss: 0.1145\n",
      "[Epoch:58000/100000] Loss: 0.1084\n",
      "[Epoch:59000/100000] Loss: 0.1026\n",
      "[Epoch:60000/100000] Loss: 0.0973\n",
      "[Epoch:61000/100000] Loss: 0.0924\n",
      "[Epoch:62000/100000] Loss: 0.0878\n",
      "[Epoch:63000/100000] Loss: 0.0835\n",
      "[Epoch:64000/100000] Loss: 0.0796\n",
      "[Epoch:65000/100000] Loss: 0.0759\n",
      "[Epoch:66000/100000] Loss: 0.0725\n",
      "[Epoch:67000/100000] Loss: 0.0693\n",
      "[Epoch:68000/100000] Loss: 0.0663\n",
      "[Epoch:69000/100000] Loss: 0.0635\n",
      "[Epoch:70000/100000] Loss: 0.0609\n",
      "[Epoch:71000/100000] Loss: 0.0584\n",
      "[Epoch:72000/100000] Loss: 0.0561\n",
      "[Epoch:73000/100000] Loss: 0.054\n",
      "[Epoch:74000/100000] Loss: 0.0519\n",
      "[Epoch:75000/100000] Loss: 0.05\n",
      "[Epoch:76000/100000] Loss: 0.0482\n",
      "[Epoch:77000/100000] Loss: 0.0465\n",
      "[Epoch:78000/100000] Loss: 0.0449\n",
      "[Epoch:79000/100000] Loss: 0.0434\n",
      "[Epoch:80000/100000] Loss: 0.042\n",
      "[Epoch:81000/100000] Loss: 0.0406\n",
      "[Epoch:82000/100000] Loss: 0.0393\n",
      "[Epoch:83000/100000] Loss: 0.0381\n",
      "[Epoch:84000/100000] Loss: 0.037\n",
      "[Epoch:85000/100000] Loss: 0.0359\n",
      "[Epoch:86000/100000] Loss: 0.0348\n",
      "[Epoch:87000/100000] Loss: 0.0338\n",
      "[Epoch:88000/100000] Loss: 0.0329\n",
      "[Epoch:89000/100000] Loss: 0.0319\n",
      "[Epoch:90000/100000] Loss: 0.0311\n",
      "[Epoch:91000/100000] Loss: 0.0302\n",
      "[Epoch:92000/100000] Loss: 0.0295\n",
      "[Epoch:93000/100000] Loss: 0.0287\n",
      "[Epoch:94000/100000] Loss: 0.028\n",
      "[Epoch:95000/100000] Loss: 0.0273\n",
      "[Epoch:96000/100000] Loss: 0.0266\n",
      "[Epoch:97000/100000] Loss: 0.026\n",
      "[Epoch:98000/100000] Loss: 0.0254\n",
      "[Epoch:99000/100000] Loss: 0.0248\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100000\n",
    "total = 0\n",
    "correct = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    prediction = model(x)\n",
    "    loss = criterion(prediction, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'[Epoch:{epoch}/{epochs}] Loss: {round(loss.item(), 4)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
